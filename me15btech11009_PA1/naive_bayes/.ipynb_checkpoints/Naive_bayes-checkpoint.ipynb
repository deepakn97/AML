{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes Classifier\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "eps = 1e-9\n",
    "#for reading and organising data\n",
    "class getData():\n",
    "    def __init__(self):\n",
    "        self.train_x = []\n",
    "        self.train_y = []\n",
    "        self.test_x = []\n",
    "        self.test_y = []\n",
    "    \n",
    "    def getTrainData(self,cpath):\n",
    "        for file in os.listdir(cpath+\"/EmailsData/spam-train\"):\n",
    "            with open(cpath+\"/EmailsData/spam-train/\"+file, 'r') as f:\n",
    "                self.train_x.append(f.read())\n",
    "            self.train_y.append(1)\n",
    "        for file in os.listdir(cpath+\"/EmailsData/nonspam-train\"):\n",
    "            with open(cpath+\"/EmailsData/nonspam-train/\"+file, 'r') as f:\n",
    "                self.train_x.append(f.read())\n",
    "            self.train_y.append(0)\n",
    "            \n",
    "    def getTestData(self,cpath):\n",
    "        for file in os.listdir(cpath+\"/EmailsData/spam-test\"):\n",
    "            with open(cpath+\"/EmailsData/spam-test/\"+file, 'r') as f:\n",
    "                self.test_x.append(f.read())\n",
    "            self.test_y.append(1)\n",
    "        for file in os.listdir(cpath+\"/EmailsData/nonspam-test\"):\n",
    "            with open(cpath+\"/EmailsData/nonspam-test/\"+file, 'r') as f:\n",
    "                self.test_x.append(f.read())\n",
    "            self.test_y.append(0)\n",
    "\n",
    "class Nbayes():\n",
    "    def __init__(self):\n",
    "        self.vec = TfidfVectorizer()\n",
    "        self.selK = SelectKBest(mutual_info_classif, k = 50)\n",
    "        self.gauss = GaussianNB()\n",
    "        self.predictions = []\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        self.train_y = []\n",
    "        self.test_y = []\n",
    "        self.mean_s = []\n",
    "        self.mean_ns = []\n",
    "        self.std_s = []\n",
    "        self.std_ns = []\n",
    "    \n",
    "    def reduce_train(self,train_x, train_y):\n",
    "        #reduce train data using Tfidf vectoriser\n",
    "        self.train_y = train_y\n",
    "        self.train = self.vec.fit_transform(train_x)\n",
    "        #selecting k best feautres\n",
    "        self.train = self.selK.fit_transform(self.train,train_y)\n",
    "        \n",
    "    def reduce_test(self, test_x, test_y):\n",
    "        #reduce test data\n",
    "        self.test_y = test_y\n",
    "        test = self.vec.transform(test_x)\n",
    "        #selecting k best feautres\n",
    "        test = self.selK.transform(test)\n",
    "        self.test = test\n",
    "    \n",
    "    def train_own(self):\n",
    "        #train own model\n",
    "        self.train = np.array(self.train.todense())\n",
    "        \n",
    "        #calculating mean and standard deviation array for spam\n",
    "        self.mean_s = np.mean(self.train[:350], axis = 0)\n",
    "        self.std_s = np.std(self.train[:350],axis = 0)\n",
    "        \n",
    "        #calculating mean and standard deviation array for non-spam\n",
    "        self.mean_ns = np.mean(self.train[350:], axis=0)\n",
    "        self.std_ns = np.std(self.train[350:], axis = 0)\n",
    "    \n",
    "    def predict_own(self):\n",
    "        self.predictions = []\n",
    "        \n",
    "        #converting sparse matrix to dense for calculation\n",
    "        self.test = np.array(self.test.todense())\n",
    "        \n",
    "        #calculating probabilities using gaussian distribution on each feature, while ignoring points where std = 0\n",
    "        #ignored 1/sqrt(2*pi) term on every point because it is irrelevent for comparison\n",
    "        prob_s = self.test-self.mean_s\n",
    "        prob_ns = self.test-self.mean_ns\n",
    "        for j in range(260):\n",
    "            for i in range(50):\n",
    "                if(self.std_s[i]-0.0 <= eps):\n",
    "                    prob_s[j][i] = 1\n",
    "                else:\n",
    "                    prob_s[j][i] = (prob_s[j][i]*prob_s[j][i])/(2.0*self.std_s[i]*self.std_s[i])\n",
    "                    prob_s[j][i] = (np.exp((-1)*prob_s[j][i]))/(1.0*self.std_s[i])\n",
    "        \n",
    "        for j in range(260):\n",
    "            for i in range(50):\n",
    "                if(self.std_ns[i]-0.0 <= eps):\n",
    "                    prob_ns[j][i] = 1\n",
    "                else:\n",
    "                    prob_ns[j][i] = (prob_ns[j][i]*prob_ns[j][i])/(2.0*self.std_ns[i] * self.std_ns[i])\n",
    "                    prob_ns[j][i] = (np.exp((-1)*prob_ns[j][i]))/(1.0*self.std_ns[i])\n",
    "        \n",
    "        #taking product of all probabilities\n",
    "        s = np.prod(prob_s,axis=1)\n",
    "        ns = np.prod(prob_ns,axis=1)\n",
    "        \n",
    "        #comparing probabilities of being spam and non spam, if they are equiprobable then we'll classify it as spam\n",
    "        for i in range(260):\n",
    "            if(s[i] >= ns[i]):\n",
    "                self.predictions.append(1)\n",
    "            else:\n",
    "                self.predictions.append(0)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        return (self.getScores(self.predictions))\n",
    "    \n",
    "    #Library implementation of Naive bayes\n",
    "    def NBlib(self):\n",
    "        self.gauss.fit(self.train.todense(),self.train_y)\n",
    "        self.predictions = self.gauss.predict(self.test.todense())\n",
    "        self.predictions = self.predictions.tolist()\n",
    "        return (self.getScores(self.predictions))\n",
    "    \n",
    "    #calculating true positives, false positives, true negatives, false negatives\n",
    "    def getScores(self,predict):\n",
    "        tp = 0.0\n",
    "        tn = 0.0\n",
    "        fn = 0.0\n",
    "        fp = 0.0\n",
    "        for i in range(260):\n",
    "            if(predict[i]==1 and self.test_y[i]==1):\n",
    "                tp+=1.0\n",
    "            elif(predict[i]==1 and self.test_y[i]==0):\n",
    "                fp+=1.0\n",
    "            elif(predict[i]==0 and self.test_y[i]==0):\n",
    "                tn+=1.0\n",
    "            elif(predict[i]==0 and self.test_y[i]==1):\n",
    "                fn+=1.0\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        f1 = 2/(1/precision + 1/recall)\n",
    "        accuracy = (tp+tn)/260.0\n",
    "        return f1,accuracy,tp,fp,tn,fn\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score\n",
      "Own Implementation:  0.9230769230769231  Library Implementation:  0.942084942084942\n",
      "\n",
      "Accuracy\n",
      "Own Implementation:  92.3076923076923  Library Implementation:  94.23076923076923\n",
      "\n",
      "                             Confusion Matrix\n",
      "                      Positive Class       Negative Class   \n",
      "Positive Predicted:     120.0                   10.0\n",
      "Positive Predicted:     10.0                   120.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = getData()\n",
    "    data.getTrainData(os.getcwd())\n",
    "    data.getTestData(os.getcwd())\n",
    "    \n",
    "    #training of model\n",
    "    bayes = Nbayes()\n",
    "    #reducing train data to 700x50 by choosing 50 best features\n",
    "    bayes.reduce_train(data.train_x,data.train_y)\n",
    "    bayes.reduce_test(data.test_x,data.test_y)\n",
    "    \n",
    "    f1_lib,acc_lib,tp,fp,tn,fn = bayes.NBlib()\n",
    "    \n",
    "    bayes.train_own()\n",
    "    f1_own,acc_own,tp,fp,tn,fn = bayes.predict_own()\n",
    "        \n",
    "    print(\"F1 score\")\n",
    "    print(\"Own Implementation: \", f1_own, \" Library Implementation: \", f1_lib)\n",
    "    print(\"\\nAccuracy\")\n",
    "    print(\"Own Implementation: \", acc_own*100, \" Library Implementation: \", acc_lib*100)\n",
    "    print(\"\\n                             Confusion Matrix\")\n",
    "    print(\"                      Positive Class   \",\"   Negative Class   \")\n",
    "    print(\"Positive Predicted:    \", tp, \"                 \", fp)\n",
    "    print(\"Positive Predicted:    \", fn, \"                 \", tn)    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences:\n",
    "1. Both F1 score and Accuracy si reported more on Scikit learn implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
