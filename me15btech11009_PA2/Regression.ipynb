{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "This notebook contains codes for Ridge regression on a random dataset. Error function was optimised using scipy.optimise.minimsize\n",
    "Sklearn fucntion for Lasso regression was used for comparing the results of lasso and ridge regression on same dataset(random). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(m,d):\n",
    "#     set seed\n",
    "    np.random.seed(35)\n",
    "#     generate random matrix of size (150,75) from standard normal distribution\n",
    "    X = np.random.randn(m,d)\n",
    "    \n",
    "#     generate true weight vector(random standard normal)\n",
    "    t_theta = np.zeros((d,1))\n",
    "    t_theta[0:10:2,0] = [10 for i in range(5)]\n",
    "    t_theta[1:10:2,0] = [-10 for i in range(5)]\n",
    "    \n",
    "#     generate noise vector(mean = 0,standard deviation = 0.1)\n",
    "    epsilon = np.random.randn(m,1)*0.1\n",
    "    \n",
    "#     make true label vector\n",
    "    y = np.matmul(X,t_theta) + epsilon\n",
    "        \n",
    "    return(X,t_theta,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "\n",
    "data,t_theta,y = make_data(150,75)\n",
    "\n",
    "# splitting data into train,test and validation sets\n",
    "train_x = data[:80,:]\n",
    "val_x = data[80:100,:]\n",
    "test_x = data[100:150,:]\n",
    "\n",
    "train_y = y[:80,:]\n",
    "val_y = y[80:100,:]\n",
    "test_y = y[100:150,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_theta(alpha):\n",
    "    \n",
    "#     calculate theta using closed form solution\n",
    "    l = alpha\n",
    "    theta = np.linalg.inv(np.matmul(train_x.T,train_x) + l * np.identity(len(train_x[0])))\n",
    "    theta = np.matmul(theta,train_x.T)\n",
    "    theta = np.matmul(theta,train_y)\n",
    "    \n",
    "    return(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(alpha):\n",
    "    \n",
    "#     calculate dummy y\n",
    "\n",
    "#     calculate regularized loss value\n",
    "    return(sum((val_y - np.matmul(val_x,get_theta(alpha)))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum error will be achieved with aplha =  0.000934517964759\n",
      "Final loss for alpha = opt_alpha will be:  10.8002447855\n",
      "Final loss for alpha = opt_alpha for built in ridge regression will be:  10.6781499511\n"
     ]
    }
   ],
   "source": [
    "# calculate optimal alpha value for ridge regression\n",
    "\n",
    "# choose an initial guess for alpha\n",
    "initial_guess = 10\n",
    "opt_alpha = minimize(get_loss,initial_guess)\n",
    "print(\"Minimum error will be achieved with aplha = \", opt_alpha.x[0])\n",
    "\n",
    "# get final loss\n",
    "final_theta = get_theta(opt_alpha.x[0])\n",
    "loss = sum((test_y - np.matmul(test_x,final_theta))**2)\n",
    "print(\"Final loss for alpha = opt_alpha will be: \", loss[0])\n",
    "\n",
    "# built-in ridge regression\n",
    "built_ridge = Ridge(alpha = opt_alpha.x,random_state = 25)\n",
    "built_ridge.fit(train_x,train_y)\n",
    "prediction = built_ridge.predict(test_x)\n",
    "built_loss = sum((test_y-prediction)**2)\n",
    "print(\"Final loss for alpha = opt_alpha for built in ridge regression will be: \", built_loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report for Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before setting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights having true value zero but estimated to be non-zero:  65.0\n",
      "Number of weights having true value non-zero but estimated to be zero:  0.0\n"
     ]
    }
   ],
   "source": [
    "true_zero = final_theta[10:]\n",
    "true_nonzero = final_theta[:10]\n",
    "\n",
    "true_zero[true_zero != 0] = 1\n",
    "true_zero[true_zero == 0] = 0\n",
    "\n",
    "true_nonzero[true_nonzero == 0] = 1\n",
    "true_nonzero[true_nonzero != 0] = 0\n",
    "\n",
    "print(\"Number of weights having true value zero but estimated to be non-zero: \", sum(true_zero)[0])\n",
    "print(\"Number of weights having true value non-zero but estimated to be zero: \", sum(true_nonzero)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After setting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of weights having true value zero but estimated to be non-zero:  0.0\n",
      "Number of weights having true value non-zero but estimated to be zero:  0.0\n"
     ]
    }
   ],
   "source": [
    "threshold = 1\n",
    "final_theta = get_theta(opt_alpha.x)\n",
    "final_theta[abs(final_theta) < threshold] = 0\n",
    "true_zero = final_theta[10:]\n",
    "true_nonzero = final_theta[:10]\n",
    "\n",
    "true_zero[true_zero != 0] = 1\n",
    "true_zero[true_zero == 0] = 0\n",
    "\n",
    "true_nonzero[true_nonzero == 0] = 1\n",
    "true_nonzero[true_nonzero != 0] = 0\n",
    "\n",
    "print(\"Number of weights having true value zero but estimated to be non-zero: \", sum(true_zero)[0])\n",
    "print(\"Number of weights having true value non-zero but estimated to be zero: \", sum(true_nonzero)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss for alpha = opt_alpha for built in lasso regression will be:  0.0175061631706\n"
     ]
    }
   ],
   "source": [
    "# built-in lasso regression\n",
    "built_lasso = Lasso(alpha = opt_alpha.x,random_state = 25)\n",
    "built_lasso.fit(train_x,train_y)\n",
    "prediction = built_lasso.predict(test_x)\n",
    "built_loss = sum((test_y.T-prediction)**2)\n",
    "print(\"Final loss for alpha = opt_alpha for built in lasso regression will be: \", built_loss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "1. Lasso regression is performing better than ridge. One of the reason for the same is, Lasso inherently performs feature selection as it can make coefficients zero since it uses direct sum of error terms. \n",
    "2. Accuracy increases after setting threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
